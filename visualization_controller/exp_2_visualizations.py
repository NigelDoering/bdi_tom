"""Visualization utilities for Experiment 2 metrics.

Generates plots describing distractor probabilities across observation
fractions and peak distractor susceptibility for each evaluated model.
Outputs are written to the same directory as the summary JSON.
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Render Experiment 2 evaluation plots.")
    parser.add_argument(
        "--summary",
        type=Path,
        default=Path("data/simulation_data/run_1/visualizations/exp_2/exp_2_summary.json"),
        help="Path to exp_2_summary.json generated by exp_2_eval.py.",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Optional directory for plots (defaults to summary parent directory).",
    )
    return parser.parse_args()


def load_summary(summary_path: Path) -> Dict[str, object]:
    with summary_path.open("r", encoding="utf-8") as fh:
        data = json.load(fh)
    return data


def fractions_and_models(summary: Dict[str, object]) -> Tuple[List[float], List[Dict[str, object]]]:
    results = summary["results"]
    fractions = [float(f) for f in results[0]["fractions"].keys()] if results else []
    fractions.sort()
    return fractions, results


def plot_with_ci(
    output_path: Path,
    title: str,
    ylabel: str,
    fractions: List[float],
    series: Dict[str, Dict[float, Dict[str, float]]],
    metric_key: str,
) -> None:
    plt.figure(figsize=(8, 5))
    x_ticks = [f * 100 for f in fractions]

    for model_name, fraction_data in series.items():
        means = [fraction_data[f][metric_key]["mean"] for f in fractions]
        ci_low = [fraction_data[f][metric_key]["ci_low"] for f in fractions]
        ci_high = [fraction_data[f][metric_key]["ci_high"] for f in fractions]

        lower_err = [max(0.0, m - l) if l is not None else 0.0 for m, l in zip(means, ci_low)]
        upper_err = [max(0.0, u - m) if u is not None else 0.0 for m, u in zip(means, ci_high)]

        plt.plot(x_ticks, means, marker="o", linewidth=2, label=model_name)
        plt.fill_between(x_ticks, [m - le for m, le in zip(means, lower_err)], [m + ue for m, ue in zip(means, upper_err)], alpha=0.2)

    plt.title(title)
    plt.xlabel("Trajectory Observed (%)")
    plt.ylabel(ylabel)
    plt.grid(alpha=0.3)
    plt.xticks(x_ticks)
    plt.legend()
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()


def plot_peak(output_path: Path, results: List[Dict[str, object]]) -> None:
    plt.figure(figsize=(6, 5))
    model_names = [entry["model"] for entry in results]
    means = [entry["peak_distractor"]["mean"] for entry in results]
    ci_low = [entry["peak_distractor"]["ci_low"] for entry in results]
    ci_high = [entry["peak_distractor"]["ci_high"] for entry in results]

    lower_err = [max(0.0, m - l) if l is not None else 0.0 for m, l in zip(means, ci_low)]
    upper_err = [max(0.0, u - m) if u is not None else 0.0 for m, u in zip(means, ci_high)]

    x_pos = range(len(model_names))
    plt.bar(x_pos, means, yerr=[lower_err, upper_err], capsize=6, alpha=0.8)
    plt.xticks(x_pos, model_names, rotation=20)
    plt.ylabel("Peak Distractor Probability")
    plt.title("Peak Distractor Susceptibility")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()


def main() -> None:
    args = parse_args()
    summary = load_summary(args.summary)

    output_dir = args.output or args.summary.parent
    output_dir.mkdir(parents=True, exist_ok=True)

    fractions, results = fractions_and_models(summary)
    fraction_series = {
        entry["model"]: {
            float(frac): metrics
            for frac, metrics in entry["fractions"].items()
        }
        for entry in results
    }

    metric_specs = [
        ("distractor", "exp_2_distractor_probs.png", "Distractor Probability vs. Trajectory Observation", "Distractor Probability"),
        ("goal", "exp_2_goal_probs.png", "True Goal Probability vs. Trajectory Observation", "Goal Probability"),
        ("top1", "exp_2_top1_accuracy.png", "Top-1 Accuracy vs. Trajectory Observation", "Top-1 Accuracy"),
        ("top5", "exp_2_top5_accuracy.png", "Top-5 Accuracy vs. Trajectory Observation", "Top-5 Accuracy"),
        ("brier", "exp_2_brier_score.png", "Brier Score vs. Trajectory Observation", "Brier Score"),
    ]

    for metric_key, filename, title, ylabel in metric_specs:
        plot_with_ci(
            output_dir / filename,
            title,
            ylabel,
            fractions,
            fraction_series,
            metric_key=metric_key,
        )

    plot_peak(output_dir / "exp_2_peak_distractor.png", results)

    print(f"Saved Experiment 2 plots to {output_dir}")


if __name__ == "__main__":
    main()
